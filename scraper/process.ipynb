{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Tag\n",
    "from pathlib import Path\n",
    "from typing import TypedDict, List, Tuple, Dict\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import RegexpParser, Tree\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "class Subject(TypedDict):\n",
    "    name: str\n",
    "    keywords: List[str]\n",
    "\n",
    "class Major(TypedDict):\n",
    "    code: str\n",
    "    subjects: List[Subject]\n",
    "\n",
    "cwd = Path().resolve()\n",
    "result_path = cwd.joinpath('result')\n",
    "\n",
    "result : List[Major] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/barcode/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/barcode/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/barcode/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/barcode/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/barcode/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "\n",
    "# spacy.prefer_gpu()\n",
    "# spacy.require_gpu()\n",
    "spacy.require_cpu()\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_sentence(sentence: str) -> List[str]:\n",
    "#     sentence = nltk.sent_tokenize(sentence)\n",
    "#     tokenized_sentence = [nltk.word_tokenize(s) for s in sentence]\n",
    "#     tagged_sentence = [nltk.pos_tag(s) for s in tokenized_sentence]\n",
    "\n",
    "#     grammar = (\"NP: {<DT>?<JJ>*<NN>}\")\n",
    "#     chunker = RegexpParser(grammar)\n",
    "\n",
    "#     phrases: List[str] = []\n",
    "\n",
    "#     for tag in tagged_sentence:\n",
    "#         tree:Tree = chunker.parse(tag)\n",
    "#         for subtree in tree.subtrees():\n",
    "#             if subtree.label() == \"NP\":\n",
    "#                 phrases.append(subtree._pformat_flat(\"\", \"()\", False))\n",
    "\n",
    "#     return phrases\n",
    "\n",
    "# def spacy_processor(text: str) -> List[str]:\n",
    "#     nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#     doc = nlp(text)\n",
    "#     phrases : List[str] = []\n",
    "\n",
    "#     for noun in doc.noun_chunks:\n",
    "#         phrases.append(noun.text)\n",
    "\n",
    "#     return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_banlist = set()\n",
    "subject_banlist = set()\n",
    "\n",
    "with open('keyword_banlist.txt') as f:\n",
    "    keyword_banlist.update([word.lower() for word in f.read().split(\"\\n\")])\n",
    "\n",
    "with open('subject_banlist.txt') as f:\n",
    "    subject_banlist.update([word.lower() for word in f.read().split(\"\\n\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_major(major: str) -> Major:\n",
    "    global subject_banlist, keyword_banlist\n",
    "    numbering_pattern = f\"\\(\\d+\\)|\\d+\\)|\\d+\\.|\\([a-zA-Z]\\)\"\n",
    "    # stop_words = set(stopwords.words('english'))\n",
    "    # tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "    # lemmatizer = WordNetLemmatizer()\n",
    "    result_path = Path().resolve().joinpath('result')\n",
    "    wajib = result_path.joinpath(f\"{major}/wajib\")\n",
    "    pilihan = result_path.joinpath(f\"{major}/pilihan\")\n",
    "\n",
    "    subjects = [wajib.joinpath(filename) for filename in os.listdir(wajib.__str__())] + [pilihan.joinpath(filename) for filename in os.listdir(pilihan.__str__())]\n",
    "\n",
    "    subjects_list: List[Subject] = []\n",
    "\n",
    "    subject_text: List[Tuple[str, Dict[str, str]]] = []\n",
    "\n",
    "    for subject in subjects:\n",
    "        with open(subject, \"r\", encoding=\"utf-8\") as f:\n",
    "            # print(f\"reading {subject}\")\n",
    "            txt = f.read().encode('utf-8', errors='ignore').decode('utf-8')\n",
    "            soup = BeautifulSoup(txt)\n",
    "\n",
    "            table = soup.find(\"tbody\")\n",
    "\n",
    "            if table is not None:\n",
    "                rows: List[Tag] = table.find_all(\"tr\")\n",
    "\n",
    "                name = rows[3].find_all(\"td\")[1].text\n",
    "\n",
    "                if name.lower() in subject_banlist:\n",
    "                    continue\n",
    "\n",
    "                syllable = rows[5].find_all(\"td\")[1].text\n",
    "                syllable  = re.sub(numbering_pattern, \" \", syllable)\n",
    "                syllable = re.sub(\"\\s+\", \" \", syllable)\n",
    "                # syllable = re.sub(\"\\\\u\\d\\d\\d\\d|\\*\", \"\", syllable)\n",
    "                # syllable = syllable.translate(string.)\n",
    "                syllable = re.sub(\"[^0-9a-zA-Z ]+\", \"\", syllable)\n",
    "                syllable = syllable.encode(\"ascii\", errors='ignore').__str__()\n",
    "                syllable = re.sub(\"b'\\s*\", \"\", syllable)\n",
    "\n",
    "                subject_text.append((syllable, {\"subject_name\": name}))\n",
    "\n",
    "                # nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "                # doc = nlp(syllable)\n",
    "                # phrases : List[str] = []\n",
    "\n",
    "                # for noun in doc.noun_chunks:\n",
    "                #     phrases.append(noun.text)\n",
    "\n",
    "                # remove punctuation and stopwords\n",
    "                # words = [lemmatizer.lemmatize(word.lower()) for word in tokenizer.tokenize(syllable) if word.lower() not in stop_words]\n",
    "\n",
    "                # subjects_list.append(Subject(name=name, keywords=phrases))\n",
    "            else:\n",
    "                print(f\"Fail to find table on file {subject}\")\n",
    "\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    docs = nlp.pipe(subject_text, n_process=2, batch_size=100, as_tuples=True)\n",
    "    stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "    for doc, context in docs:\n",
    "        name = context[\"subject_name\"]\n",
    "\n",
    "        phrases : set[str] = set()\n",
    "\n",
    "        for noun in doc.noun_chunks:\n",
    "            text = \" \".join([word for word in noun.lemma_.lower().split() if word not in stopwords])\n",
    "            \n",
    "            if text != \"\" and text not in keyword_banlist:\n",
    "                phrases.add(text)\n",
    "\n",
    "        subjects_list.append(Subject(name=name, keywords=list(phrases)))\n",
    "\n",
    "    return Major(code=major, subjects=subjects_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pool = Pool(cpu_count())\n",
    "\n",
    "majors = os.listdir(result_path.__str__())\n",
    "\n",
    "# result.append(process_major(majors[0]))\n",
    "\n",
    "for major in majors:\n",
    "    result.append(process_major(major))\n",
    "\n",
    "# result.extend(pool.map(process_major, majors[:4])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"major_subjects_keyword.json\", \"w\") as w:\n",
    "    json.dump(result, w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
